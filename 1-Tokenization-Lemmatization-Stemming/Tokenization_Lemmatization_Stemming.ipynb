{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Course-1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "homZePwZibmB",
        "TaSGfA2njSkR",
        "vN4Bvw7EjcVB",
        "wZg22k1OjtjI",
        "uToVpECLkBw6",
        "TOTxlzsOkKIm",
        "R8QpxBXSk9fq",
        "tzPJIC4Zl-aM",
        "dCytah3olpar",
        "xAOMI7bTmJEw",
        "Nzd1jj7smhiI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tokenization"
      ],
      "metadata": {
        "id": "homZePwZibmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of breaking text into smaller pieces called tokens. These smaller pieces can be sentences, words, or sub-words. For example, the sentence “I won” can be tokenized into two word-tokens “I” and “won”."
      ],
      "metadata": {
        "id": "pSXivA7BifBy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Fk_T2fu-h3rX"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (TreebankWordTokenizer,\n",
        "                           word_tokenize,\n",
        "                           wordpunct_tokenize,\n",
        "                           TweetTokenizer,\n",
        "                           MWETokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAcwFg_kkT3B",
        "outputId": "c80facd1-82bd-4756-84f8-6cb353a095b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Today we would leanr about tokenization. Are you all ready?\""
      ],
      "metadata": {
        "id": "gNtEhuX4jDgi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Whitespace tokenization"
      ],
      "metadata": {
        "id": "TaSGfA2njSkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the most simple and commonly used form of tokenization. It splits the text whenever it finds whitespace characters."
      ],
      "metadata": {
        "id": "oo6Ax14bjhxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Whitespace tokenization = {sentence.split()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZYlaWYhjMhh",
        "outputId": "69a78bbd-6e81-493a-f322-5ddc4fef5346"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace tokenization = ['Today', 'we', 'would', 'leanr', 'about', 'tokenization.', 'Are', 'you', 'all', 'ready?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Punctuation-based tokenization"
      ],
      "metadata": {
        "id": "vN4Bvw7EjcVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation-based tokenization is slightly more advanced than whitespace-based tokenization since it splits on whitespace and punctuations and also retains the punctuations."
      ],
      "metadata": {
        "id": "8U462psAjqTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Punctuation-based tokenization = {wordpunct_tokenize(sentence)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zy1CD7DjaBu",
        "outputId": "dbc1b8c1-df81-4d33-c4cb-640efc0d51f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation-based tokenization = ['Today', 'we', 'would', 'leanr', 'about', 'tokenization', '.', 'Are', 'you', 'all', 'ready', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C. Default/TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "wZg22k1OjtjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The default tokenization method in NLTK involves tokenization using regular expressions as defined in the Penn Treebank (based on English text). It assumes that the text is already split into sentences."
      ],
      "metadata": {
        "id": "pCEuuQUtj0N9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "print(f'Default/Treebank tokenization = {tokenizer.tokenize(sentence)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXcIvJmWjs1F",
        "outputId": "3f561833-d747-4b2b-f56b-3c753feaa03c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default/Treebank tokenization = ['Today', 'we', 'would', 'leanr', 'about', 'tokenization.', 'Are', 'you', 'all', 'ready', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### D. TweetTokenizer\n"
      ],
      "metadata": {
        "id": "uToVpECLkBw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Special texts, like Twitter tweets, have a characteristic structure and the generic tokenizers mentioned above fail to produce viable tokens when applied to these datasets."
      ],
      "metadata": {
        "id": "3vbM8ezQkHVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "print(f'Tweet-rules based tokenization = {tokenizer.tokenize(sentence)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvVIe9qCkBe2",
        "outputId": "1e1984be-d3d8-4520-ea43-35b00f9047a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet-rules based tokenization = ['Today', 'we', 'would', 'leanr', 'about', 'tokenization', '.', 'Are', 'you', 'all', 'ready', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### E. MWETokenizer"
      ],
      "metadata": {
        "id": "TOTxlzsOkKIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The multi-word expression tokenizer is a rule-based, “add-on” tokenizer offered by NLTK. Once the text has been tokenized by a tokenizer of choice, some tokens can be re-grouped into multi-word expressions."
      ],
      "metadata": {
        "id": "P2PbyQbikOjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('Martha', 'Jones'))\n",
        "print(f'Multi-word expression (MWE) tokenization = {tokenizer.tokenize(word_tokenize(sentence))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVBTdUVJkJjm",
        "outputId": "3841b10d-e273-4a4e-ab12-867a2681c1b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-word expression (MWE) tokenization = ['Today', 'we', 'would', 'leanr', 'about', 'tokenization', '.', 'Are', 'you', 'all', 'ready', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "R8QpxBXSk9fq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Stemming "
      ],
      "metadata": {
        "id": "tzPJIC4Zl-aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. Often when searching text for a certain keyword, it helps if the search returns variations of the word. For instance, searching for “boat” might also return “boats” and “boating”. Here, “boat” would be the stem for [boat, boater, boating, boats]. Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached."
      ],
      "metadata": {
        "id": "-dAIMOT2lolC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### i) Porter Stemmer"
      ],
      "metadata": {
        "id": "dCytah3olpar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the toolkit and the full Porter Stemmer library\n",
        "import nltk\n",
        "\n",
        "from nltk.stem.porter import *\n",
        "p_stemmer = PorterStemmer()\n",
        "words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "for word in words:\n",
        "    print(word+' --> '+p_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM0IMRWFkQub",
        "outputId": "80480df2-1d6a-43c4-a1dd-ec99ac1b7023"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run --> run\n",
            "runner --> runner\n",
            "running --> run\n",
            "ran --> ran\n",
            "runs --> run\n",
            "easily --> easili\n",
            "fairly --> fairli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ii) Snowball Stemmer"
      ],
      "metadata": {
        "id": "xAOMI7bTmJEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# The Snowball Stemmer requires that you pass a language parameter\n",
        "s_stemmer = SnowballStemmer(language='english')\n",
        "words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "for word in words:\n",
        "    print(word+' --> '+s_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WDQHF1AmIcr",
        "outputId": "2e93cfcc-6984-459f-c0d6-f041f0cf1fc6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run --> run\n",
            "runner --> runner\n",
            "running --> run\n",
            "ran --> ran\n",
            "runs --> run\n",
            "easily --> easili\n",
            "fairly --> fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In this case, the stemmer performed the same as the Porter Stemmer, with the exception that it handled the stem of “fairly” more appropriately with “fair”"
      ],
      "metadata": {
        "id": "utPKZ5Z-mZ6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Lemmatization "
      ],
      "metadata": {
        "id": "Nzd1jj7smhiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In contrast to stemming, lemmatization looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words. The lemma of ‘was’ is ‘be’ and the lemma of ‘mice’ is ‘mouse’."
      ],
      "metadata": {
        "id": "elACB29CmnbP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is typically seen as much more informative than simple stemming, which is why Spacy has opted to only have Lemmatization available instead of Stemming"
      ],
      "metadata": {
        "id": "Rx7ORlQOmn2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform standard imports:\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "def show_lemmas(text):\n",
        "    for token in text:\n",
        "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
      ],
      "metadata": {
        "id": "Ca1bQ-GfmPWv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u\"I saw eighteen mice today!\")\n",
        "\n",
        "show_lemmas(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGcUZSO6mq10",
        "outputId": "5ad09ccb-ee14-47ad-8f2e-80eae8c497b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            PRON   561228191312463089     -PRON-\n",
            "saw          VERB   11925638236994514241   see\n",
            "eighteen     NUM    9609336664675087640    eighteen\n",
            "mice         NOUN   1384165645700560590    mouse\n",
            "today        NOUN   11042482332948150395   today\n",
            "!            PUNCT  17494803046312582752   !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "T6O0xUqomsbG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}